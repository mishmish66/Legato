{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from einops import einsum, pack, rearrange, repeat\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from data.npz\n",
    "data = np.load('data.npz')\n",
    "torchify = lambda *xs: [torch.tensor(x, dtype=torch.float32, device='cuda') for x in xs]\n",
    "# torchify = lambda *xs: [torch.tensor(x, dtype=torch.float32) for x in xs]\n",
    "observations, actions = torchify(data['observations'], data['actions'])\n",
    "# observations, actions = observations[:4096], actions[:4096]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(nn.Module):\n",
    "    def __init__(self, input_dim, layer_sizes, output_dim):\n",
    "        super().__init__()\n",
    "        layer_sizes = [input_dim] + layer_sizes + [output_dim]\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                nn.Linear(layer_sizes[i], layer_sizes[i + 1])\n",
    "                for i in range(len(layer_sizes) - 1)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = torch.relu(layer(x))\n",
    "        x = self.layers[-1](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransitionModel(nn.Module):\n",
    "    def __init__(self, act_dim, state_dim, latent_dim, pe_wavelength_range=[1, 1024]):\n",
    "        super().__init__()\n",
    "        n_layers = 3\n",
    "        self.sa_layers = nn.ModuleList(\n",
    "            [\n",
    "                nn.MultiheadAttention(latent_dim, 4, batch_first=True)\n",
    "                for _ in range(n_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.up_scales = nn.ModuleList(\n",
    "            [nn.Linear(latent_dim, latent_dim * 4) for _ in range(n_layers)]\n",
    "        )\n",
    "        self.down_scales = nn.ModuleList(\n",
    "            [nn.Linear(latent_dim * 4, latent_dim) for _ in range(n_layers)]\n",
    "        )\n",
    "        self.up_scale = nn.Linear(state_dim + act_dim, latent_dim)\n",
    "        self.down_scale = nn.Linear(latent_dim, state_dim)\n",
    "\n",
    "        self.pe_wavelength_range = pe_wavelength_range\n",
    "\n",
    "    def forward(self, initial_state, actions, start_indices=None, return_mask=False):\n",
    "        # Concatenate actions to initial_state\n",
    "        x = torch.cat([repeat(initial_state, \"... e -> ... r e\", r=actions.shape[-2]), actions], dim=-1)\n",
    "        x = torch.relu(self.up_scale(x))\n",
    "\n",
    "        if start_indices is None:\n",
    "            start_indices = torch.zeros(\n",
    "                initial_state.shape[0], dtype=torch.long, device=initial_state.device\n",
    "            )\n",
    "\n",
    "        embed_dim = x.shape[-1]\n",
    "        # Do a log range of frequencies\n",
    "        pe_freqs = 1 / torch.logspace(\n",
    "            np.log(self.pe_wavelength_range[0]),\n",
    "            np.log(self.pe_wavelength_range[1]),\n",
    "            embed_dim // 2,\n",
    "            base=2,\n",
    "            device=x.device,\n",
    "        )\n",
    "        batch_size = x.shape[0]\n",
    "        seq_len = actions.shape[-2]\n",
    "        # Compute the positional encoding\n",
    "        times = torch.arange(seq_len, device=x.device) - start_indices[..., None]\n",
    "        pe_freq_mat = einsum(pe_freqs, times, \"w, ... i -> i w\")\n",
    "        pe = torch.cat([torch.sin(pe_freq_mat), torch.cos(pe_freq_mat)], dim=-1)\n",
    "        x = x + pe\n",
    "\n",
    "        # Make a mask out to mask out the past\n",
    "        mask = torch.zeros(batch_size, seq_len, device=x.device, dtype=torch.bool)\n",
    "        mask[torch.arange(batch_size), start_indices] = True\n",
    "        mask = ~mask.cumsum(dim=-1) > 0\n",
    "\n",
    "        big_mask = repeat(mask, \"i seq ... -> (i heads) seq seq_also ...\", heads=4, seq_also=seq_len)\n",
    "\n",
    "        for up_scale, sa_layer, down_scale in zip(\n",
    "            self.up_scales, self.sa_layers, self.down_scales\n",
    "        ):\n",
    "            x = x + sa_layer(x, x, x, attn_mask=big_mask, need_weights=False)[0]\n",
    "            z = torch.relu(up_scale(x))\n",
    "            x = x + down_scale(z)\n",
    "\n",
    "        x = self.down_scale(x)\n",
    "\n",
    "        if return_mask:\n",
    "            return x, mask\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define networks\n",
    "\n",
    "state_dim = observations.shape[-1]\n",
    "action_dim = actions.shape[-1]\n",
    "\n",
    "state_encoder = Perceptron(state_dim, [32, 32, 32], state_dim).cuda()\n",
    "action_encoder = Perceptron(action_dim + state_dim, [32, 32, 32], action_dim).cuda()\n",
    "transition_model = TransitionModel(2, 4, 64).cuda()\n",
    "state_decoder = Perceptron(state_dim, [32, 32, 32], state_dim).cuda()\n",
    "action_decoder = Perceptron(action_dim + state_dim, [32, 32, 32], action_dim).cuda()\n",
    "\n",
    "nets = [\n",
    "    state_encoder,\n",
    "    action_encoder,\n",
    "    transition_model,\n",
    "    state_decoder,\n",
    "    action_decoder,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransitionLoss(nn.Module):\n",
    "    def __init__(self, loss_fn=nn.MSELoss()):\n",
    "        super().__init__()\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def forward(self, latent_fut_states_prime, latent_fut_states_gt, mask):\n",
    "        masked_latent_fut_states_prime = torch.where(\n",
    "            mask[..., None], latent_fut_states_gt, latent_fut_states_prime\n",
    "        )\n",
    "        loss = self.loss_fn(masked_latent_fut_states_prime, latent_fut_states_gt)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothnessLoss(nn.Module):\n",
    "    def __init__(self, norm_p=1, discount=0.99):\n",
    "        super().__init__()\n",
    "        self.discount = discount\n",
    "        self.norm_p = norm_p\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        latent_actions,\n",
    "        latent_next_states,\n",
    "        latent_actions_perturbed,\n",
    "        latent_next_states_perturbed,\n",
    "        mask,\n",
    "    ):\n",
    "        action_diffs = latent_actions_perturbed - latent_actions\n",
    "        action_dists = torch.norm(action_diffs, p=self.norm_p, dim=-1)\n",
    "\n",
    "        state_diffs = latent_next_states_perturbed - latent_next_states\n",
    "        state_dists = torch.norm(state_diffs, p=self.norm_p, dim=-1)\n",
    "\n",
    "        future_indices = torch.cumsum(~mask, dim=-1, dtype=torch.float32)\n",
    "        future_discounts = self.discount**future_indices\n",
    "        dist_limits = action_dists / future_discounts\n",
    "        state_violations = torch.relu(state_dists - dist_limits)\n",
    "        losses = state_violations**2\n",
    "        masked_losses = torch.where(mask, 0, losses)\n",
    "\n",
    "        return masked_losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoverageLoss(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_space_size,\n",
    "        action_space_size,\n",
    "        latent_samples=2048,\n",
    "        space_ball_p=1,\n",
    "        selection_tail_size=4,\n",
    "        far_sample_count=64,\n",
    "        pushing_sample_size=4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.state_space_size = state_space_size\n",
    "        self.action_space_size = action_space_size\n",
    "\n",
    "        self.latent_samples = latent_samples\n",
    "        self.space_ball_p = space_ball_p\n",
    "        self.selection_tail_size = selection_tail_size\n",
    "        self.far_sample_count = far_sample_count\n",
    "        self.pushing_sample_size = pushing_sample_size\n",
    "\n",
    "    def forward(self, latent_states, latent_actions):\n",
    "        latent_states = rearrange(latent_states, \"... e -> (...) e\")\n",
    "        latent_actions = rearrange(latent_actions, \"... e -> (...) e\")\n",
    "\n",
    "        state_norms = torch.norm(latent_states, p=self.space_ball_p, dim=-1)\n",
    "        action_norms = torch.norm(latent_actions, p=self.space_ball_p, dim=-1)\n",
    "\n",
    "        state_violations = torch.relu(state_norms - self.state_space_size)\n",
    "        action_violations = torch.relu(action_norms - self.action_space_size)\n",
    "\n",
    "        state_size_violations = state_violations**2\n",
    "        action_size_violations = action_violations**2\n",
    "\n",
    "        state_size_loss = state_size_violations.mean()\n",
    "        action_size_loss = action_size_violations.mean()\n",
    "\n",
    "        # penalize for empty space within the state space\n",
    "        # Sample random points in the latent space\n",
    "        if self.space_ball_p != 1:\n",
    "            raise NotImplementedError(\"Only L1 norm is supported :(\")\n",
    "\n",
    "        state_space_samples = (\n",
    "            torch.rand(\n",
    "                self.latent_samples,\n",
    "                latent_states.shape[-1],\n",
    "                device=latent_states.device,\n",
    "            )\n",
    "            * 2\n",
    "            - 1\n",
    "        ) * self.state_space_size\n",
    "        action_space_samples = (\n",
    "            torch.rand(\n",
    "                self.latent_samples,\n",
    "                latent_actions.shape[-1],\n",
    "                device=latent_actions.device,\n",
    "            )\n",
    "            * 2\n",
    "            - 1\n",
    "        ) * self.action_space_size\n",
    "\n",
    "        # Find the state_space that is the farthest from any of the latent_states\n",
    "        state_space_dists = torch.cdist(state_space_samples, latent_states, p=1)\n",
    "        state_space_dist_tail = (\n",
    "            state_space_dists.sort(dim=-1)\n",
    "            .values[:, : self.selection_tail_size]\n",
    "            .mean(dim=-1)\n",
    "        )\n",
    "        farthest_idxs = state_space_dist_tail.argsort(descending=True)[\n",
    "            : self.far_sample_count\n",
    "        ]\n",
    "        farthest_state_samples = state_space_samples[farthest_idxs]\n",
    "\n",
    "        action_space_dists = torch.cdist(action_space_samples, latent_actions, p=1)\n",
    "        action_space_dist_tail = (\n",
    "            action_space_dists.sort(dim=-1)\n",
    "            .values[:, : self.selection_tail_size]\n",
    "            .mean(dim=-1)\n",
    "        )\n",
    "        farthest_idxs = action_space_dist_tail.argsort(descending=True)[\n",
    "            : self.far_sample_count\n",
    "        ]\n",
    "        farthest_action_samples = action_space_samples[farthest_idxs]\n",
    "\n",
    "        # Now make the states by the farthest latent states closer to the farthest samples\n",
    "        # Maybe in the future make just the few closest ones closer\n",
    "        empty_state_space_dists = torch.cdist(\n",
    "            farthest_state_samples, latent_states, p=1\n",
    "        )\n",
    "        close_empty_state_space_dists = empty_state_space_dists.sort(dim=-1).values[\n",
    "            :, : self.pushing_sample_size\n",
    "        ]\n",
    "        state_coverage_losses = close_empty_state_space_dists**2\n",
    "\n",
    "        empty_action_space_dists = torch.cdist(\n",
    "            farthest_action_samples, latent_actions, p=1\n",
    "        )\n",
    "        close_empty_action_space_dists = empty_action_space_dists.sort(dim=-1).values[\n",
    "            :, : self.pushing_sample_size\n",
    "        ]\n",
    "        action_coverage_losses = close_empty_action_space_dists**2\n",
    "\n",
    "        return (\n",
    "            state_size_loss\n",
    "            + action_size_loss\n",
    "            + state_coverage_losses.mean()\n",
    "            + action_coverage_losses.mean()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_310693/1386447883.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state_batch = observations[[torch.tensor(inds) for inds in encoder_batch_inds]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, total loss: 5.028264999389648, lr: [0.00995], transition loss: 0.3088099956512451, smoothness loss: 1.4508627543818875e-07, coverage loss: 0.32831481099128723, state reconstruction loss: 4.0420355796813965, action reconstruction loss: 0.34910470247268677\n",
      "Iteration 64, total loss: 2.071960926055908, lr: [0.007219385759785157], transition loss: 0.022135138511657715, smoothness loss: 2.125751450421376e-07, coverage loss: 0.18434357643127441, state reconstruction loss: 1.7123594284057617, action reconstruction loss: 0.1531226933002472\n",
      "Iteration 128, total loss: 0.6194939017295837, lr: [0.005238143793828013], transition loss: 0.03545884042978287, smoothness loss: 6.955904154892778e-07, coverage loss: 0.12750311195850372, state reconstruction loss: 0.4347015619277954, action reconstruction loss: 0.021829720586538315\n",
      "Iteration 192, total loss: 0.4290889799594879, lr: [0.003800621177172762], transition loss: 0.041192058473825455, smoothness loss: 1.1217654218853568e-06, coverage loss: 0.11052003502845764, state reconstruction loss: 0.2667837142944336, action reconstruction loss: 0.010592035949230194\n",
      "Iteration 256, total loss: 0.42571118474006653, lr: [0.0027576030557606972], transition loss: 0.038810987025499344, smoothness loss: 2.138066975021502e-06, coverage loss: 0.10770013928413391, state reconstruction loss: 0.2718154191970825, action reconstruction loss: 0.007382479961961508\n",
      "Iteration 320, total loss: 0.4465007185935974, lr: [0.0020008241439094273], transition loss: 0.037217192351818085, smoothness loss: 8.828430509311147e-06, coverage loss: 0.10095749795436859, state reconstruction loss: 0.2997899055480957, action reconstruction loss: 0.008527268655598164\n",
      "Iteration 384, total loss: 0.40649834275245667, lr: [0.0014517307871732713], transition loss: 0.04082247614860535, smoothness loss: 2.766747229543398e-06, coverage loss: 0.10240644216537476, state reconstruction loss: 0.2586510181427002, action reconstruction loss: 0.0046156467869877815\n"
     ]
    }
   ],
   "source": [
    "action_mse = nn.MSELoss()\n",
    "state_mse = nn.MSELoss()\n",
    "transition_loss_func = TransitionLoss()\n",
    "\n",
    "smoothness_loss_func = SmoothnessLoss()\n",
    "\n",
    "coverage_loss_func = CoverageLoss(\n",
    "    state_space_size=1.5,\n",
    "    action_space_size=1.75,\n",
    "    # latent_state_samples=1024,\n",
    "    # latent_action_samples=1024,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [param for net in nets for param in net.parameters()], lr=1e-2\n",
    ")\n",
    "# lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "#     optimizer, start_factor=1, end_factor=1e-2, total_iters=1024\n",
    "# )\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
    "    optimizer, gamma=0.995, last_epoch=-1,\n",
    ")\n",
    "\n",
    "encoder_batch_size = 1024\n",
    "transition_batch_size = 32\n",
    "\n",
    "for i in range(1024):\n",
    "\n",
    "    # Yoink a batch of data\n",
    "    encoder_batch_raveled_inds = torch.randperm(np.prod(observations.shape[:-1]))[\n",
    "        :encoder_batch_size\n",
    "    ]\n",
    "    encoder_batch_inds = torch.unravel_index(\n",
    "        encoder_batch_raveled_inds, observations.shape[:-1]\n",
    "    )\n",
    "\n",
    "    transition_traj_batch_inds = torch.randperm(observations.shape[0], device=\"cuda\")[\n",
    "        :transition_batch_size\n",
    "    ]\n",
    "    transition_start_inds = torch.randint(\n",
    "        0, int(observations.shape[-2] // 1.1), (transition_batch_size,), device=\"cuda\"\n",
    "    )\n",
    "\n",
    "    state_batch = observations[[torch.tensor(inds) for inds in encoder_batch_inds]]\n",
    "    action_batch = actions[encoder_batch_inds]\n",
    "\n",
    "    starting_states = observations[transition_traj_batch_inds, transition_start_inds]\n",
    "    state_traj_batch = observations[transition_traj_batch_inds]\n",
    "    action_traj_batch = actions[transition_traj_batch_inds]\n",
    "\n",
    "    # Now do a forward pass\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    latent_states = state_encoder(state_batch)\n",
    "    latent_actions = action_encoder(torch.cat([action_batch, state_batch], dim=-1))\n",
    "\n",
    "    reconstructed_states = state_decoder(latent_states)\n",
    "    reconstructed_actions = action_decoder(\n",
    "        torch.cat([latent_actions, latent_states], dim=-1)\n",
    "    )\n",
    "\n",
    "    state_reconstruction_loss = state_mse(reconstructed_states, state_batch)\n",
    "    action_reconstruction_loss = action_mse(reconstructed_actions, action_batch)\n",
    "\n",
    "    latent_start_states = state_encoder(starting_states)\n",
    "    latent_traj_actions = action_encoder(\n",
    "        torch.cat([action_traj_batch, state_traj_batch], dim=-1)\n",
    "    )\n",
    "    latent_fut_states_prime, mask = transition_model(\n",
    "        latent_start_states,\n",
    "        latent_traj_actions,\n",
    "        start_indices=transition_start_inds,\n",
    "        return_mask=True,\n",
    "    )\n",
    "    latent_fut_states_gt = state_encoder(state_traj_batch)\n",
    "\n",
    "    perturbations = torch.randn_like(latent_traj_actions)\n",
    "    perturbations = perturbations / torch.norm(perturbations, p=1, dim=-1, keepdim=True)\n",
    "    perturbations = perturbations * torch.rand(\n",
    "        (*perturbations.shape[:-1], 1), device=\"cuda\"\n",
    "    )\n",
    "\n",
    "    latent_traj_actions_perturbed = latent_traj_actions + perturbations\n",
    "    # Normalize the perturbations if they are too large\n",
    "    perturbed_action_norms = torch.norm(\n",
    "        latent_traj_actions_perturbed, p=1, dim=-1, keepdim=True\n",
    "    )\n",
    "    latent_traj_actions_perturbed = torch.where(\n",
    "        perturbed_action_norms > coverage_loss_func.action_space_size,\n",
    "        latent_traj_actions_perturbed\n",
    "        * coverage_loss_func.action_space_size\n",
    "        / perturbed_action_norms,\n",
    "        latent_traj_actions_perturbed,\n",
    "    )\n",
    "    latent_fut_states_prime_perturbed = transition_model(\n",
    "        latent_start_states,\n",
    "        latent_traj_actions_perturbed,\n",
    "        start_indices=transition_start_inds,\n",
    "    )\n",
    "\n",
    "    transition_loss = transition_loss_func(\n",
    "        latent_fut_states_prime, latent_fut_states_gt, mask\n",
    "    )\n",
    "    smoothness_loss = smoothness_loss_func(\n",
    "        latent_actions,\n",
    "        latent_fut_states_prime,\n",
    "        latent_traj_actions_perturbed,\n",
    "        latent_fut_states_prime_perturbed,\n",
    "        mask,\n",
    "    )\n",
    "    coverage_loss = coverage_loss_func(latent_states, latent_actions) * 0.01\n",
    "\n",
    "    loss = (\n",
    "        state_reconstruction_loss\n",
    "        + action_reconstruction_loss\n",
    "        + transition_loss\n",
    "        + smoothness_loss\n",
    "        + coverage_loss\n",
    "    )\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "    if i % 64 == 0:\n",
    "        print(\n",
    "            f\"Iteration {i}, total loss: {loss.item()}, lr: {lr_scheduler.get_last_lr()}, transition loss: {transition_loss.item()}, smoothness loss: {smoothness_loss.item()}, coverage loss: {coverage_loss.item()}, state reconstruction loss: {state_reconstruction_loss.item()}, action reconstruction loss: {action_reconstruction_loss.item()}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stdev of encoded states and actions across each element\n",
    "with torch.no_grad():\n",
    "    encoded_states = rearrange(state_encoder(observations), \"... e -> (...) e\")\n",
    "    state_std = torch.std(encoded_states, dim=0)\n",
    "\n",
    "    encoded_actions = rearrange(\n",
    "        action_encoder(torch.cat([actions, observations], dim=-1)), \"... e -> (...) e\"\n",
    "    )\n",
    "    \n",
    "    recovered_states = state_decoder(encoded_states)\n",
    "    recovered_actions = action_decoder(torch.cat([encoded_actions, encoded_states], dim=-1))\n",
    "    \n",
    "    action_std = torch.std(encoded_actions, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.1045, 0.1314, 0.1917, 0.2648], device='cuda:0'),\n",
       " tensor([0.8885, 0.8942], device='cuda:0'))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_std, action_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(\n",
    "    tensor([0.4034, 0.8038, 0.6741, 0.7653], device=\"cuda:0\"),\n",
    "    tensor([1.0077, 1.0054], device=\"cuda:0\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0330,  0.0288,  0.0944,  0.1882],\n",
       "         [-0.0304,  0.0295,  0.1164, -0.0094],\n",
       "         [-0.0265,  0.0345,  0.0065, -0.1179],\n",
       "         ...,\n",
       "         [ 0.0164, -0.0168, -0.7266, -0.0929],\n",
       "         [ 0.0190,  0.0030, -0.7976, -0.3223],\n",
       "         [ 0.0162, -0.0236, -0.5972,  0.0540]], device='cuda:0'),\n",
       " tensor([[ 0.0602, -0.0286],\n",
       "         [-0.0025, -0.0470],\n",
       "         [ 0.0062,  0.0016],\n",
       "         ...,\n",
       "         [ 0.1599, -0.1029],\n",
       "         [-0.0293,  0.0160],\n",
       "         [-0.0170,  0.0015]], device='cuda:0'))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recovered_states - rearrange(observations, \"... e -> (...) e\"), recovered_actions - rearrange(actions, \"... e -> (...) e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3718,  1.4048],\n",
       "         [ 2.8031,  2.7743],\n",
       "         [ 0.0139, -0.0805],\n",
       "         [ 0.0267, -0.1615]],\n",
       "\n",
       "        [[ 1.3733,  1.4038],\n",
       "         [ 2.8042,  2.7747],\n",
       "         [ 0.0139, -0.1025],\n",
       "         [ 0.0267,  0.0362]],\n",
       "\n",
       "        [[ 1.3773,  1.4038],\n",
       "         [ 2.8106,  2.7761],\n",
       "         [ 0.0140,  0.0076],\n",
       "         [ 0.0269,  0.1447]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.9412,  1.9248],\n",
       "         [-0.6199, -0.6031],\n",
       "         [-0.0166,  0.7100],\n",
       "         [-0.0083,  0.0847]],\n",
       "\n",
       "        [[ 1.9516,  1.9326],\n",
       "         [-0.5970, -0.5999],\n",
       "         [-0.0159,  0.7817],\n",
       "         [-0.0088,  0.3135]],\n",
       "\n",
       "        [[ 1.9546,  1.9384],\n",
       "         [-0.6242, -0.6006],\n",
       "         [-0.0165,  0.5807],\n",
       "         [-0.0084, -0.0624]]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([recovered_states, rearrange(observations, \"... e -> (...) e\")], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorPolicy(nn.Module):\n",
    "    def __init__(self, action_dim, horizon=128, iters=64):\n",
    "        super().__init__()\n",
    "        self.action_dim = action_dim\n",
    "        self.horizon = horizon\n",
    "        self.iters = iters\n",
    "\n",
    "    def forward(self, state, target_state, prev_latent_action_plan=None):\n",
    "        if prev_latent_action_plan is None:\n",
    "            prev_latent_action_plan = torch.randn(\n",
    "                state.shape[0], self.horizon, self.action_dim, device=state.device\n",
    "            )\n",
    "            prev_latent_action_plan = prev_latent_action_plan / torch.norm(\n",
    "                prev_latent_action_plan, p=1, dim=-1, keepdim=True\n",
    "            )\n",
    "            prev_latent_action_plan = prev_latent_action_plan * torch.rand(\n",
    "                (*prev_latent_action_plan.shape[:-1], 1), device=state.device\n",
    "            )\n",
    "            prev_latent_action_plan = (\n",
    "                prev_latent_action_plan * coverage_loss_func.action_space_size\n",
    "            )\n",
    "\n",
    "        latent_action_plan = prev_latent_action_plan.clone()\n",
    "\n",
    "        latent_state = state_encoder(state)\n",
    "        latent_target_state = state_encoder(target_state)\n",
    "\n",
    "        optim = torch.optim.Adam([latent_action_plan], lr=1e-2)\n",
    "\n",
    "        for i in range(self.iters):\n",
    "            optim.zero_grad()\n",
    "            latent_fut_states = transition_model(latent_state, latent_action_plan)\n",
    "            loss = state_mse(latent_fut_states, latent_target_state)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "        next_action = action_decoder(\n",
    "            torch.cat([latent_action_plan, latent_state], dim=-1)\n",
    "        )\n",
    "        return next_action, latent_action_plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mishmish/Documents/venvs/legato_simple/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 4])) that is different to the input size (torch.Size([1, 128, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m latent_action_plan \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1024\u001b[39m):\n\u001b[0;32m---> 27\u001b[0m     next_action, latent_action_plan \u001b[38;5;241m=\u001b[39m \u001b[43mactor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_action_plan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/venvs/legato_simple/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/venvs/legato_simple/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 34\u001b[0m, in \u001b[0;36mActorPolicy.forward\u001b[0;34m(self, state, target_state, prev_latent_action_plan)\u001b[0m\n\u001b[1;32m     32\u001b[0m     latent_fut_states \u001b[38;5;241m=\u001b[39m transition_model(latent_state, latent_action_plan)\n\u001b[1;32m     33\u001b[0m     loss \u001b[38;5;241m=\u001b[39m state_mse(latent_fut_states, latent_target_state)\n\u001b[0;32m---> 34\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     optim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     37\u001b[0m next_action \u001b[38;5;241m=\u001b[39m action_decoder(\n\u001b[1;32m     38\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcat([latent_action_plan, latent_state], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     39\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/venvs/legato_simple/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/venvs/legato_simple/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "# Now let's optimize a trajectory\n",
    "\n",
    "state_encoder.eval()\n",
    "action_encoder.eval()\n",
    "transition_model.eval()\n",
    "state_decoder.eval()\n",
    "action_decoder.eval()\n",
    "\n",
    "initial_state = observations[0, 0]\n",
    "\n",
    "actions = torch.randn(32, 64, 2, device=\"cuda\")\n",
    "actions = actions / torch.norm(actions, p=1, dim=-1, keepdim=True)\n",
    "actions = (\n",
    "    actions\n",
    "    * torch.rand((32, 64, 1), device=\"cuda\")\n",
    "    * coverage_loss_func.action_space_size\n",
    ")\n",
    "\n",
    "target_state = observations[0, -1]\n",
    "\n",
    "actor = ActorPolicy(2, 128).cuda()\n",
    "\n",
    "env = gym.make(\"PointMaze_Large-v3\")\n",
    "latent_action_plan = None\n",
    "\n",
    "for i in range(1024):\n",
    "    next_action, latent_action_plan = actor(initial_state[None], target_state[None], latent_action_plan)\n",
    "    print(f\"Iteration {i}, loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legato_simple",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
